

# ğŸš€ 100 Days of Machine Learning Journey

[![Language](https://img.shields.io/badge/Python-3.8%2B-blue.svg?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Library](https://img.shields.io/badge/Scikit--Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)
[![Tools](https://img.shields.io/badge/Jupyter-Notebook-orange?style=for-the-badge&logo=jupyter&logoColor=white)](https://jupyter.org/)
[![Status](https://img.shields.io/badge/Status-Completed-success?style=for-the-badge)](https://github.com/Hanifullah313)

> *"Learning is not attained by chance, it must be sought for with ardor and attended to with diligence."* â€” Abigail Adams

---

## ğŸ“– Overview

Welcome to the comprehensive documentation of my **100 Days of Machine Learning** challenge. This repository serves as a centralized knowledge base, archiving my daily progress, code implementations, theoretical notes, and hands-on projects as I mastered the landscape of Artificial Intelligence from scratch.

The goal of this journey was not just to write code, but to understand the **mathematical intuition** behind algorithms, master **data preprocessing** techniques, and implement robust **predictive models** without relying solely on black-box libraries.

---

## ğŸ§  Key Learning Modules

This curriculum is structured to build a strong foundation before advancing to complex algorithms.

### ğŸ”¹ Phase 1: Data Engineering & Preprocessing
*Before any model can learn, the data must be understood.*
- **Data Handling & Preparation:** Mastering Pandas for data manipulation and profiling.
- **Handling Missing Data & Outliers:** Techniques for imputation (Mean/Median/Mode, KNN Imputer) and outlier detection (Z-Score, IQR).
- **Feature Engineering:** Transforming raw data into meaningful features (One-Hot Encoding, Ordinal Encoding, Scaling).

### ğŸ”¹ Phase 2: Supervised Learning (Regression & Classification)
*Teaching machines to map inputs to outputs.*
- **Linear Regression:** Understanding relationships between continuous variables, Gradient Descent, and Cost Functions.
- **Logistic Regression:** Binary classification, Sigmoid functions, and decision boundaries.
- **Support Vector Machines (SVM):** Hyperplanes, Kernels (RBF, Polynomial), and margin maximization.
- **Naive Bayes:** Probabilistic classifiers based on Bayes' Theorem (Gaussian, Multinomial, Bernoulli).
- **K-Nearest Neighbors (KNN):** Distance metrics (Euclidean, Manhattan) and the curse of dimensionality.
- **Decision Trees:** Entropy, Information Gain, Gini Impurity, and pruning techniques.

### ğŸ”¹ Phase 3: Unsupervised Learning
*Finding hidden patterns in unlabeled data.*
- **K-Means Clustering:** Centroid initialization, the Elbow Method, and Silhouette Scores.
- **Agglomerative Clustering:** Hierarchical clustering, Dendrograms, and linkage criteria.

### ğŸ”¹ Phase 4: Ensemble Learning & Advanced Techniques
*Combining weak learners to create a strong predictor.*
- **Ensemble Learning:** Bagging, Boosting, and Random Forests.
- **Stacking & Blending:** Meta-models and multi-layer model architectures.
- **Voting Classifiers:** Hard vs. Soft voting probabilities.

---

## ğŸ“‚ Repository Structure

```plaintext
100-Days-of-Machine-Learning-Journey/
â”‚
â”œâ”€â”€ ğŸ“‚ Linear Regression/          # Foundation of predictive modeling
â”œâ”€â”€ ğŸ“‚ Logistic Regression/        # Classification basics
â”œâ”€â”€ ğŸ“‚ Decision Trees/             # Tree-based algorithms
â”œâ”€â”€ ğŸ“‚ SVM/                        # Support Vector Machines
â”œâ”€â”€ ğŸ“‚ KNN/                        # Distance-based classification
â”œâ”€â”€ ğŸ“‚ naive Bayes/                # Probabilistic algorithms
â”œâ”€â”€ ğŸ“‚ KMeans Clustering/          # Partition-based clustering
â”œâ”€â”€ ğŸ“‚ Agglomerative Clustering/   # Hierarchical clustering
â”œâ”€â”€ ğŸ“‚ Ensemble Learning/          # Random Forest, Adaboost, etc.
â”œâ”€â”€ ğŸ“‚ Stacking and blending/      # Advanced model architecture
â”œâ”€â”€ ğŸ“‚ Feature Engineering/        # Encoding, Scaling, Transformations
â”œâ”€â”€ ğŸ“‚ Data Handling/              # Pandas Profiling & EDA
â””â”€â”€ ğŸ“œ README.md                   # Project Documentation

```

---

## ğŸ› ï¸ Tech Stack & Tools

| Category | Tools Used |
| --- | --- |
| **Language** | Python ğŸ |
| **Data Manipulation** | Pandas, NumPy |
| **Visualization** | Matplotlib, Seaborn |
| **Machine Learning** | Scikit-Learn |
| **Environment** | Jupyter Notebook, VS Code |

---

## ğŸ“Š Sample Visualizations

Throughout this journey, I focused heavily on visualizing data distributions and model boundaries. Here are examples of the types of analysis included in the notebooks:

* **Correlation Heatmaps** to identify feature relationships.
* **Scatter Plots** with decision boundaries for SVM and Logistic Regression.
* **Dendrograms** for Hierarchical Clustering.
* **Elbow Curves** for determining optimal  in K-Means.

---

## ğŸ¤ Contributing

While this is a personal learning repository, suggestions and improvements are always welcome! If you spot a mathematical error or a code optimization:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“¬ Contact

**Hanif Ullah**

---

<p align="center">
<i>If you found this repository helpful, please consider giving it a â­!</i>
</p>

```

```
